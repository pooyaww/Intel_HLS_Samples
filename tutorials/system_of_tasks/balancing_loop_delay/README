This tutorial shows how capacity balancing plays a role in designing functional,
performant hardware using a system of tasks. This tutorial is intended as a
follow-up to balancing_pipeline_latency.

This tutorial uses a simple design, with two parallel paths, as shown in the
following diagram, similar to balancing_pipeline_latency.

           + Input
           |
           |
+----------v---------+
|      splitter      |
+------+---------+---+
       |         |
       |         +----------+
       |                    |
+------v-------------+      |
|     delay_block    |      |     Side Path
+------+-------------+      |(Internal Stream)
       |                    |
       |         +----------+
       |         |
+------v---------v---+
|      merger        |
+----------+---------+
           |
           |
           v Output

The key difference here is that the latency in the delay_block task is comes
from the function's structure (mainly the loops) whereas the latency in
balancing_pipeline_latency was created by the compiler as it pipelined the
datapath of the "Filter" task.

In this design, the "delay_block" task delays writing out the data it reads from
the "splitter task" by pushing it through a shift register.  Although the
"delay_block" task contains very simple logic without much pipelining, the code
sends data a few iterations (specifically 16 iterations) after the first piece
of data is received. This delay is explicitly expressed in the code, not
generated by the compiler, and you can see how much delay to expect by reviewing
the code. You should have enough capacity in the side path to match this delay.

If the capacity of the side path is smaller than the delay, the side path fills
up and stalls the upstream "splitter" task before "delay_block" starts sending
data downstream. Because the "Merger" task depends on data from both the
"Filter" task and the side path, it cannot consume data in the side path to free
it up. Since the side path is full, the "splitter" task cannot send more data to
the "delay_block" task because the "delay_block" task is being stalled. The
"delay_block" task cannot run further iterations, so it never reaches the point
when it can send data downstream. This creates a deadlock, and the entire system
stalls.

In this tutorial, part_1 has no capacity on the side path. In this case, the
deadlock appears and the simulation will hang indefinitely. In part_2, the
capacity of the side path is set to 20. In this case, "delay_block" runs
continuously. The reason why 20 was chosen (over 16) is to account for a few
additional cycles of latency due to the surrounding datapath in the
"delay_block". An interesting experiment (left to the user) is to vary the
SIDE_FIFO_CAPACITY between 16-20 and see the impact on performance. You will
notice that the design will not hang, but will see a slight decrease in overall
latency.

To use this tutorial:
1) Run the tutorial.
2) Examine the report in part_2_balanced.prj
   Check the verification statistics, note the final output stream "comb_out" is
   sending data close to 1 word per cycle.
3) Run part_1_unbalanced which will enter a deadlock.
   Kill the cosim program after a few minutes, and check the simulation waveform
   using "vsim part_1_unbalanced.prj/verification/vsim.wlf".
4) Check the streaming interface of the top level component, see that it is stalling.

This tutorial requires the following tools to be installed:
  - i++
  - ModelSim

To run this tutorial:
  - On Linux run "make"
  - On Windows run "build"

